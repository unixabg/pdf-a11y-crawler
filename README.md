# ðŸ“„ pdf-a11y-crawler

`pdf-a11y-crawler` is a lightweight command-line tool for discovering PDF files on a website and identifying **potential accessibility issues**, with a focus on detecting **image-only PDFs** and other high-risk cases.

This tool is intended as an **early-stage accessibility screening utility**, not a compliance certification tool.

---

## ðŸŽ¯ Purpose

This project helps answer a common question:

> **â€œWhich PDFs on this site are most likely inaccessible?â€**

It does this by:
- Crawling a web page (optionally recursively)
- Discovering linked PDF files
- Detecting whether PDFs contain extractable text
- Optionally running PDF/UA checks using veraPDF
- Producing machine-readable reports (CSV / JSON)

---

## âœ… What This Tool Does

âœ” Finds PDF files linked from a webpage
âœ” Detects image-only (scanned) PDFs
âœ” Identifies whether text is present
âœ” Optionally runs veraPDF for PDF/UA checks
âœ” Generates CSV and JSON reports
âœ” Supports dry-run and recursive scanning

---

## âŒ What This Tool Does NOT Do

âŒ It does **not** certify WCAG or ADA compliance
âŒ It does **not** guarantee PDF accessibility
âŒ It does **not** fix PDFs
âŒ It does **not** replace manual accessibility review

This tool provides **signal**, not legal or accessibility certification.

---

## ðŸ§  Important Context

- WCAG 2.1 does **not** require PDF/UA compliance.
- A PDF can fail PDF/UA and still be readable.
- An image-only PDF is almost always inaccessible.
- This tool focuses on identifying **high-risk cases quickly**.

---

## ðŸ›  Setup (Debian Trixie)

This section describes how to install and run **pdf-a11y-crawler** on **Debian Trixie** using a Python virtual environment.

---

### 1ï¸âƒ£ Install System Dependencies

Update package lists:

```bash
sudo apt update
```

Install required packages:

```bash
sudo apt install -y \
    python3 \
    python3-venv \
    python3-pip \
    poppler-utils \
    curl
```

> `poppler-utils` provides `pdffonts`, which is required for detecting text in PDFs.

---

### 2ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/unixabg/pdf-a11y-crawler.git
cd pdf-a11y-crawler
```

---

### 3ï¸âƒ£ Create and Activate a Python Virtual Environment

```bash
python3 -m venv venv
source venv/bin/activate
```

You should now see:

```text
(venv) user@host:~/pdf-a11y-crawler$
```

---

### 4ï¸âƒ£ Install Python Dependencies

```bash
pip install requests beautifulsoup4 tqdm
```

---

### 5ï¸âƒ£ Make the Script Executable

```bash
chmod +x pdf-a11y-crawl.py
```

---

### 6ï¸âƒ£ Verify Installation

```bash
./pdf-a11y-crawl.py --version
```

Expected output:

```text
pdf-a11y-crawler 0.1.0
```

---

### 7ï¸âƒ£ Optional: Install veraPDF (for PDF/UA checks)

veraPDF is optional and only required if you use `--verapdf`.

Download from:

```
https://verapdf.org/software/
```

Once installed verify veraPDF installation:

```bash
verapdf --version
```

---

###  8ï¸âƒ£ Deactivate Environment (optional)

```bash
deactivate
```

---

## â–¶ï¸ Usage

### Basic scan (single page)
```bash
./pdf-a11y-crawl.py https://example.com/page
```

### Recursive scan
```bash
./pdf-a11y-crawl.py --recursive https://example.com
```

### Dry run (no downloads)
```bash
./pdf-a11y-crawl.py --dry-run https://example.com
```

### Enable PDF/UA checks
```bash
./pdf-a11y-crawl.py --verapdf https://example.com
```

### See help for more usage options
```bash
./pdf-a11y-crawl.py --help
usage: pdf-a11y-crawl [-h] [--dry-run] [--include-external-pdfs] [--max-bytes MAX_BYTES] [--max-pages MAX_PAGES] [--out OUT] [--pdftotext]
                       [--recursive] [--timeout TIMEOUT] [--verapdf] [--version]
                      url

Crawl a web page and identify PDF files, then analyze them for basic accessibility characteristics such as text presence (image-only detection) and optional PDF/UA checks.

positional arguments:
  url                   Starting URL to scan for PDF links

options:
  -h, --help            show this help message and exit
  --dry-run             Discover PDFs but do not download or analyze them
  --include-external-pdfs
                        Also scan PDFs hosted on external domains
  --max-bytes MAX_BYTES
                        Maximum size of a PDF in bytes (default: 50MB)
  --max-pages MAX_PAGES
                        Maximum number of pages to crawl when using --recursive (default: 200)
  --out OUT             Output directory (default: ./out)
  --pdftotext           Dump extracted text for review when text layer is detected
  --recursive           Follow links on the same site (default: off)
  --timeout TIMEOUT     HTTP timeout in seconds (default: 20)
  --verapdf             Run veraPDF to check PDF/UA compliance (slower)
  --version             show program's version number and exit

Examples:
  pdf-a11y-crawl https://example.com/page
  pdf-a11y-crawl --recursive https://example.com
  pdf-a11y-crawl --verapdf https://example.com/docs

Notes:
  - By default, only the given page is scanned.
  - --recursive enables crawling of linked pages.
  - --verapdf enables PDF/UA checks (optional, slower).
```

---

## ðŸ“Š Output

Results are written to:

```
out/
 â”œâ”€â”€/date-time/report.csv
 â””â”€â”€/date-time/report.json
```

Each PDF entry includes:
- Source page
- PDF URL
- Text detection result
- Font count
- Optional veraPDF result
- Notes and warnings

---

## ðŸ§ª Accessibility Detection Logic

### Primary check
- Uses `pdffonts` to determine whether text exists
- PDFs with no fonts are flagged as likely inaccessible

### Optional check
- Uses `veraPDF` to evaluate PDF/UA conformance
- Results are informational only

---

## âš ï¸ Compliance Notice

This tool:

- âŒ Does not certify WCAG or ADA compliance
- âŒ Does not replace manual accessibility testing
- âœ” Helps identify likely accessibility risks
- âœ” Supports accessibility remediation workflows

Use this tool as part of a broader accessibility review process.

---

## ðŸ” Responsible Use

Only scan websites and documents you own or are authorized to test.

Do not use this tool to scan:
- third-party sites without permission
- internal systems you do not control

---

## ðŸ“¦ Version

Current version:

```
0.1.2
```

---

## ðŸ©º Remediation & Accessibility Resources

This project is intended to help identify accessibility issues in PDFs and support efforts toward WCAG 2.1 and PDF/UA compliance. While this tool focuses on detection and analysis, remediation often requires specialized tooling and expertise.

### PDFix
[PDFix](https://pdfix.net/) provides professional-grade tools for PDF accessibility remediation and validation. Their SDK and desktop tools can help:

- Fix tagged PDF structure
- Remediate reading order and semantics
- Validate PDF/UA compliance
- Improve compatibility with screen readers

PDFix is a useful resource for teams looking to move from automated analysis toward fully accessible, standards-compliant PDFs.

> Mentioned with permission from the PDFix team.

---

## ðŸ™Œ Contributions

Suggestions, improvements, and issues are welcome.

This project is intentionally kept simple and focused.
